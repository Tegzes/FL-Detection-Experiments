{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "\n",
    "from tqdm import tqdm \n",
    "import hydra\n",
    "# from omegaconf import DictConfig\n",
    "\n",
    "from clearml import Task, Logger\n",
    "\n",
    "from datamodule import data, utils\n",
    "from models import LSTM, CNN, Bertweet, Roberta\n",
    "\n",
    "from transformers import BertModel\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, RobertaConfig, RobertaTokenizer\n",
    "\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "tokens_pt = roberta_tokenizer(\"in desperate need of and i can not stress this enough spring break\", padding='max_length', max_length=12, truncation=True, return_tensors='pt')  \n",
    "tokens_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class BERTSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,\n",
    "                 bidirectional,\n",
    "                 output_dim):\n",
    "        super(BERTSentiment, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.config = RobertaConfig.from_pretrained(\"roberta-base\")\n",
    "        self.config.output_hidden_states = True\n",
    "        self.roberta_layers = RobertaModel.from_pretrained(\"roberta-base\", config = self.config)\n",
    "        self.hidden_size = self.roberta_layers.config.hidden_size\n",
    "\n",
    "        self.LSTM = torch.nn.LSTM(self.hidden_size, 384, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.out = nn.Linear(768, output_dim)\n",
    "\n",
    "    def forward(self, text, mask, token_type_ids):\n",
    "        #text = [batch size, sent len]\n",
    "        # seq_layers, pooled_output = self.bert(text)\n",
    "        embedded = self.roberta_layers(text, mask, token_type_ids)[0]\n",
    "\n",
    "        # embedded = embedded[1]\n",
    "        lstm_output, (last_hidden, _) = self.LSTM(embedded)\n",
    "        #embedded = [batch size, emb dim]\n",
    "\n",
    "        output_hidden = torch.cat((last_hidden[0], last_hidden[1]), dim=-1)\n",
    "        output_hidden = F.dropout(output_hidden,0.2)\n",
    "        \n",
    "        # output_hidden = F.dropout(embedded, 0.2)\n",
    "        output = self.out(output_hidden)\n",
    "\n",
    "        #output = [batch size, out dim]\n",
    "        return output\n",
    "\n",
    "\n",
    "roberta =  RobertaModel.from_pretrained(\"roberta-base\")       \n",
    "model = BERTSentiment(roberta, 2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model(tokens.input_ids, tokens.attention_mask))\n",
    "input_ids = tokens_pt.input_ids.to(device)\n",
    "mask = tokens_pt.attention_mask.to(device)\n",
    "print(model(input_ids, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 97\n",
    "sarc_path = '/home/tegzes/Desktop/FL-Detection-Experiments/datamodule/isarcasm2022.csv'\n",
    "BATCH_SIZE_TRAIN = 1\n",
    "BATCH_SIZE_TEST = 1\n",
    "MAX_LEN = 256\n",
    "HIDDEN_DIM = 64\n",
    "OUTPUT_DIM = 1\n",
    "EMBEDDING_LENGTH = 300\n",
    "N_LAYERS = 2\n",
    "LEARNING_RATE = 1e-5\n",
    "BIDIRECTIONAL = False\n",
    "DROPOUT = 0.25\n",
    "N_EPOCHS = 3\n",
    "utils.seed_everything(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = data.roberta_data_loader(sarc_path, BATCH_SIZE_TRAIN, BATCH_SIZE_TEST, True, 0, MAX_LEN, roberta_tokenizer, SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "tensor([[ 0.0622, -0.0001]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "tensor(0.6624, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# for batch_idx, batch in tqdm(enumerate(train_iterator, 0)):\n",
    "batch = next(iter(train_iterator))\n",
    "        \n",
    "ids = batch['ids'].to(DEVICE, dtype = torch.long)\n",
    "mask = batch['mask'].to(DEVICE, dtype = torch.long)\n",
    "token_type_ids = batch['token_type_ids'].to(DEVICE, dtype = torch.long)\n",
    "targets = batch['targets'].to(DEVICE, dtype = torch.long)\n",
    "\n",
    "# print(ids)\n",
    "# print(mask)\n",
    "# print(token_type_ids)\n",
    "print(targets)\n",
    "outputs = model(ids, mask, token_type_ids)\n",
    "print(outputs)\n",
    "_, predictions = torch.max(outputs.data, dim = 1)\n",
    "print(predictions)\n",
    "loss = cross_entropy_loss(outputs, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "bce_loss = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "def calcuate_accuracy(preds, targets):\n",
    "    n_correct = (preds==targets).sum().item()\n",
    "    return n_correct\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task = Task.init(project_name=\"FL Detection\", task_name=\"Training\")\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    no_of_iterations = 0\n",
    "    no_of_examples = 0\n",
    "    \n",
    "    # using torch metrics to calculate the metrics\n",
    "    metric_acc = torchmetrics.Accuracy().to(torch.device(\"cuda\", 0))\n",
    "    metric_f1 = torchmetrics.F1(num_classes = 2, average=\"none\").to(torch.device(\"cuda\", 0))\n",
    "    metric_f1_micro = torchmetrics.F1(num_classes = 2).to(torch.device(\"cuda\", 0))\n",
    "    metric_f1_macro = torchmetrics.F1(num_classes = 2, average='macro').to(torch.device(\"cuda\", 0))\n",
    "    metric_precision = torchmetrics.Precision(num_classes = 2, average=\"none\").to(torch.device(\"cuda\", 0))\n",
    "    metric_recall = torchmetrics.Recall(num_classes = 2, average=\"none\").to(torch.device(\"cuda\", 0))\n",
    "  \n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, batch in tqdm(enumerate(iterator, 0)):\n",
    "        \n",
    "        ids = batch['ids'].to(DEVICE, dtype = torch.long)\n",
    "        mask = batch['mask'].to(DEVICE, dtype = torch.long)\n",
    "        token_type_ids = batch['token_type_ids'].to(DEVICE, dtype = torch.long)\n",
    "        targets = batch['targets'].to(DEVICE, dtype = torch.long)\n",
    "        # tweet_lens = batch['tweet_len']\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)#, tweet_lens)#.to('cpu'))\n",
    "        # outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        # targets = targets.unsqueeze(1) # for BCEWithLogitsLoss criterion\n",
    "        loss = criterion(outputs, targets)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        _, predictions = torch.max(outputs.data, dim = 1)\n",
    "        acc = calcuate_accuracy(predictions, targets)\n",
    "\n",
    "        # loss = criterion(predictions, targets)\n",
    "        # epoch_loss += loss.item()\n",
    "\n",
    "        no_of_iterations += 1\n",
    "        no_of_examples += targets.size(0)\n",
    "                \n",
    "        metric_acc.update(predictions, targets)\n",
    "        metric_f1.update(outputs, targets)\n",
    "        metric_f1_micro.update(outputs, targets)\n",
    "        metric_f1_macro.update(outputs, targets)\n",
    "        metric_precision.update(predictions, targets)\n",
    "        metric_recall.update(predictions, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # for GPU\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logger.current_logger().report_scalar(\n",
    "        #     \"train\", \"loss\", iteration = (epoch * len(iterator) + batch_idx), value = loss.item())\n",
    "#----\n",
    "        # print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        #         epoch, batch_idx * len(batch['ids']), len(iterator),\n",
    "        #         100. * batch_idx / len(iterator), loss.item()))\n",
    "        \n",
    "\n",
    "    epoch_loss = epoch_loss/no_of_iterations\n",
    "    epoch_acc = (acc*100)/no_of_examples\n",
    "        \n",
    "    acc_torch = metric_acc.compute()\n",
    "    print(f\"Training Accuracy: {acc_torch}\")\n",
    "    \n",
    "    f1 = metric_f1.compute()\n",
    "    print(f\"Training F1: {f1}\")\n",
    "    \n",
    "    f1_micro = metric_f1_micro.compute()\n",
    "    print(f\"Training F1 Micro: {f1_micro}\")\n",
    "\n",
    "    f1_macro = metric_f1_macro.compute()\n",
    "    print(f\"Training F1 Macro: {f1_macro}\")\n",
    " \n",
    "    precision = metric_precision.compute()\n",
    "    print(f\"Training Precision: {precision}\")\n",
    "\n",
    "    recall = metric_recall.compute()\n",
    "    print(f\"Training Recall: {recall}\")\n",
    "    \n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "  \n",
    "    # Logger.current_logger().report_scalar(\n",
    "    #     \"train\", \"accuracy\", iteration=epoch, value=acc_torch)\n",
    "\n",
    "    \n",
    "    metric_acc.reset()\n",
    "    metric_f1.reset()\n",
    "    metric_f1_micro.reset()\n",
    "    metric_f1_macro.reset()\n",
    "    metric_precision.reset()\n",
    "    metric_recall.reset()\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# evaluation routine\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    no_of_iterations = 0\n",
    "    no_of_examples = 0    \n",
    "    \n",
    "   # torch metrics\n",
    "    metric_acc = torchmetrics.Accuracy().to(torch.device(\"cuda\", 0))\n",
    "    metric_f1 = torchmetrics.F1(num_classes = 2, average=\"none\").to(torch.device(\"cuda\", 0))\n",
    "    metric_f1_micro = torchmetrics.F1(num_classes = 2).to(torch.device(\"cuda\", 0))\n",
    "    metric_f1_macro = torchmetrics.F1(num_classes = 2, average='macro').to(torch.device(\"cuda\", 0))\n",
    "    metric_precision = torchmetrics.Precision(num_classes = 2, average=\"none\").to(torch.device(\"cuda\", 0))\n",
    "    metric_recall = torchmetrics.Recall(num_classes = 2, average=\"none\").to(torch.device(\"cuda\", 0))\n",
    "  \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for _, batch in tqdm(enumerate(iterator, 0)):\n",
    "\n",
    "            ids = batch['ids'].to(DEVICE, dtype = torch.long)\n",
    "            mask = batch['mask'].to(DEVICE, dtype = torch.long)\n",
    "            token_type_ids = batch['token_type_ids'].to(DEVICE, dtype = torch.long)\n",
    "            targets = batch['targets'].to(DEVICE, dtype = torch.long)\n",
    "\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "        \n",
    "            _, predictions = torch.max(outputs.data, dim = 1)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            acc = calcuate_accuracy(predictions, targets)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "            metric_acc.update(predictions, targets)\n",
    "            metric_f1.update(outputs, targets)\n",
    "            metric_f1_micro.update(outputs, targets)\n",
    "            metric_f1_macro.update(outputs, targets)\n",
    "            metric_precision.update(predictions, targets)\n",
    "            metric_recall.update(predictions, targets)\n",
    "\n",
    "            no_of_iterations += 1\n",
    "            no_of_examples += targets.size(0)\n",
    "    \n",
    "    epoch_loss = epoch_loss/no_of_iterations\n",
    "    epoch_acc = (acc*100)/no_of_iterations #no_of_examples\n",
    "\n",
    "    # print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    #     epoch_loss, acc, len(iterator),\n",
    "    #     100. * acc / len(iterator)))\n",
    "\n",
    "    acc_torch = metric_acc.compute()\n",
    "    print(f\"Validation Accuracy: {acc_torch}\")\n",
    "    \n",
    "    f1 = metric_f1.compute()\n",
    "    print(f\"Validation F1 Validation: {f1}\")\n",
    "    \n",
    "    f1_micro = metric_f1_micro.compute()\n",
    "    print(f\"Validation F1 Micro: {f1_micro}\")\n",
    "\n",
    "    f1_macro = metric_f1_macro.compute()\n",
    "    print(f\"Validation F1 Macro: {f1_macro}\")\n",
    " \n",
    "    precision = metric_precision.compute()\n",
    "    print(f\"Validation Precision: {precision}\")\n",
    "\n",
    "    recall = metric_recall.compute()\n",
    "    print(f\"Validation Recall: {recall}\")\n",
    "    \n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "\n",
    "    # clear ml\n",
    "    # Logger.current_logger().report_scalar(\n",
    "    #     \"test\", \"loss\", iteration=epoch, value=epoch_loss)\n",
    "    # Logger.current_logger().report_scalar(\n",
    "    #     \"test\", \"accuracy\", iteration=epoch, value=acc_torch)\n",
    "\n",
    "    metric_acc.reset()\n",
    "    metric_f1.reset()\n",
    "    metric_f1_micro.reset()\n",
    "    metric_f1_macro.reset()\n",
    "    metric_precision.reset()\n",
    "    metric_recall.reset()\n",
    "             \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "# experiment loop\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, cross_entropy_loss)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, cross_entropy_loss)\n",
    "        \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, cross_entropy_loss)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "\n",
    "# task.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "353d8282a16112a98a41145f3f9c1bed3cd5174dd47df6cfdaac153128affcbe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
