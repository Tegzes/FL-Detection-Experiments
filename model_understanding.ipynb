{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "\n",
    "from tqdm import tqdm \n",
    "from transformers import AutoModel, AutoConfig\n",
    "\n",
    "import hydra\n",
    "# from omegaconf import DictConfig\n",
    "\n",
    "from clearml import Task, Logger\n",
    "\n",
    "from datamodule import data, utils\n",
    "from models import LSTM, CNN, Bertweet, Roberta\n",
    "\n",
    "from transformers import BertModel\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, RobertaConfig, RobertaTokenizer, BertTokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# roberta_tokens = roberta_tokenizer(\"in desperate need of and i can not stress this enough spring break\", padding='max_length', max_length=20, truncation=True, return_tensors='pt')\n",
    "# print(roberta_tokens)  \n",
    "\n",
    "# bert_tokens = bert_tokenizer(\"in desperate need of and i can not stress this enough spring break\", padding='max_length', max_length=20, truncation=True, return_tensors='pt')\n",
    "# print(bert_tokens) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertweetRCNN(\n",
       "  (bertweet): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(130, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (lstm): LSTM(768, 384, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (W): Linear(in_features=1536, out_features=768, bias=True)\n",
       "  (tanh): Tanh()\n",
       "  (fc): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class BertweetRCNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Bertweet Recurrent CNN\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim, dropout):\n",
    "        super(BertweetRCNN, self).__init__()\n",
    "        \n",
    "        config = AutoConfig.from_pretrained(\"vinai/bertweet-base\")\n",
    "        config.output_hidden_states = True\n",
    "        self.bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\", config)\n",
    "        self.hidden_size = self.bertweet.config.hidden_size\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(self.hidden_size, 384, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.W = torch.nn.Linear(self.hidden_size + 2*384, 768) # basically the hidden state * 2\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.fc = torch.nn.Linear(768, output_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        \n",
    "        # output_bertweet = self.bertweet(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)[0]\n",
    "        # output_embedded = batch size, seq_len, embedding_dim\n",
    "        output_lstm, _ = self.lstm(output_bertweet)\n",
    "        # output_lstm = batch size, seq_len, 2*lstm_hidden_size\n",
    "        output = torch.cat([output_lstm, output_bertweet], 2)\n",
    "        # output = batch size, seq_len, embedding_dim + 2*hidden_size\n",
    "        output = self.tanh(self.W(output)).transpose(1, 2)\n",
    "        # output = batch size, seq_len, hidden_size_linear -> batch size, hidden_size_linear, seq_len\n",
    "        output = F.max_pool1d(output, output.size(2)).squeeze(2)\n",
    "        # output = batch size, hidden_size_linear\n",
    "        output = self.fc(output)\n",
    "        # output = batch size, output_dim\n",
    "        return output\n",
    "\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.roberta_data_loader(sarc_path, BATCH_SIZE_TRAIN, BATCH_SIZE_TEST, True, 0, MAX_LEN, roberta_tokenizer, 97)\n",
    "sample = next(iter(train_iterator))\n",
    "\n",
    "ids = sample['ids'].to(DEVICE, dtype = torch.long)\n",
    "mask = sample['mask'].to(DEVICE, dtype = torch.long)\n",
    "token_type_ids = sample['token_type_ids'].to(DEVICE, dtype = torch.long)\n",
    "model = BertweetRCNN(2, 0.2)\n",
    "model.to(DEVICE)\n",
    "# make_dot(model(ids, mask, token_type_ids))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertweetRCNN(\n",
      "  (bertweet): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(130, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): RobertaPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (lstm): LSTM(768, 384, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (W): Linear(in_features=1536, out_features=768, bias=True)\n",
      "  (tanh): Tanh()\n",
      "  (fc): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased',    return_dict = True)\n",
    "\n",
    "text = \"The capital of France, \" + tokenizer.mask_token + \",contains the Eiffel Tower.\"\n",
    "\n",
    "input = tokenizer.encode_plus(text, return_tensors = \"pt\")\n",
    "\n",
    "mask_index = torch.where(input[\"input_ids\"][0] == tokenizer.mask_token_id)\n",
    "\n",
    "logits = model(**input)\n",
    "logits = logits.logits\n",
    "\n",
    "softmax = F.softmax(logits, dim = -1)\n",
    "mask_word = softmax[0, mask_index, :]\n",
    "top_word = torch.argmax(mask_word, dim=1)\n",
    "\n",
    "print(tokenizer.decode(top_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bert_tokenizer.convert_ids_to_tokens(7867))\n",
    "print(bert_tokenizer.convert_ids_to_tokens(6719))\n",
    "print(bert_tokenizer.convert_ids_to_tokens(3092))\n",
    "print(bert_tokenizer.convert_ids_to_tokens(2866))\n",
    "print(bert_tokenizer.convert_ids_to_tokens(2097))\n",
    "print(bert_tokenizer.convert_ids_to_tokens(2022))\n",
    "print(bert_tokenizer.convert_ids_to_tokens(2583))\n",
    "print(bert_tokenizer.convert_ids_to_tokens(2000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers  import BertConfig, BertForSequenceClassification\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "\n",
    "seed = 47\n",
    "seed_everything(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = data.roberta_data_loader(sarc_path, BATCH_SIZE_TRAIN, BATCH_SIZE_TEST, True, 0, MAX_LEN, roberta_tokenizer, SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iterator))\n",
    "\n",
    "# print(batch)    \n",
    "ids = batch['ids'].to(DEVICE, dtype = torch.long)\n",
    "mask = batch['mask'].to(DEVICE, dtype = torch.long)\n",
    "token_type_ids = batch['token_type_ids'].to(DEVICE, dtype = torch.long)\n",
    "targets = batch['targets'].to(DEVICE, dtype = torch.long)\n",
    "\n",
    "print(targets)\n",
    "outputs1 = model1(ids, mask, token_type_ids)\n",
    "print(outputs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss = bce_loss(outputs1, targets.unsqueeze(1).float())\n",
    "print(loss.item())\n",
    "\n",
    "_, predictions = torch.max(outputs1.data, dim = 1)\n",
    "print(predictions)\n",
    "acc1 = calcuate_accuracy(predictions, targets)\n",
    "print(acc1/BATCH_SIZE_TRAIN)\n",
    "\n",
    "print(\"MODEL 2\")\n",
    "loss = cross_entropy_loss(outputs2, targets)\n",
    "print(loss.item())\n",
    "\n",
    "_, predictions2 = torch.max(outputs2.data, dim = 1)\n",
    "print(predictions2)\n",
    "acc2 = calcuate_accuracy(predictions2, targets)\n",
    "print(acc2/BATCH_SIZE_TRAIN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = nn.Softmax(dim=1) # Or logsoftmax\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "softmax_output = probs(outputs)\n",
    "loss = bce_loss(outputs, targets)\n",
    "\n",
    "print(softmax_output)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_modell = BertClass(0.25)\n",
    "bert_modell.to(DEVICE)\n",
    "ids = bert_tokens['input_ids'].to(DEVICE, dtype = torch.long)\n",
    "mask = bert_tokens['attention_mask'].to(DEVICE, dtype = torch.long)\n",
    "token_type_ids = bert_tokens['token_type_ids'].to(DEVICE, dtype = torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = bert_modell(ids, mask, token_type_ids)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.rand(2, 3)\n",
    "print(temp)\n",
    "temp[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RobertaConfig.from_pretrained('roberta-base')\n",
    "config.output_hidden_states = True\n",
    "roberta_model = RobertaModel.from_pretrained(\"roberta-base\", config)\n",
    "# print(model)\n",
    "\n",
    "config = BertConfig.from_pretrained('bert-base-cased')\n",
    "config.output_hidden_states = True\n",
    "bert_model = BertModel.from_pretrained('bert-base-cased', config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_output_0 = bert_model(bert_tokens['input_ids'], bert_tokens['attention_mask'])[0]\n",
    "bert_output_1 = bert_model(bert_tokens['input_ids'], bert_tokens['attention_mask'])[1]\n",
    "pooler = bert_output_0[:, 0]\n",
    "\n",
    "# print(roberta_output_0)\n",
    "print(bert_output_0.shape)\n",
    "# print(roberta_output_1)\n",
    "print(bert_output_1.shape)\n",
    "# print(pooler)\n",
    "print(pooler.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Recurrent Convolutional Neural Networks for Text Classification (2015)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, hidden_size_linear, class_num, dropout):\n",
    "        super(RCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.W = nn.Linear(embedding_dim + 2*hidden_size, hidden_size_linear)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc = nn.Linear(hidden_size_linear, class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = |bs, seq_len|\n",
    "        x_emb = self.embedding(x)\n",
    "        # x_emb = |bs, seq_len, embedding_dim|\n",
    "        output, _ = self.lstm(x_emb)\n",
    "        # output = |bs, seq_len, 2*hidden_size|\n",
    "        output = torch.cat([output, x_emb], 2)\n",
    "        # output = |bs, seq_len, embedding_dim + 2*hidden_size|\n",
    "        output = self.tanh(self.W(output)).transpose(1, 2)\n",
    "        # output = |bs, seq_len, hidden_size_linear| -> |bs, hidden_size_linear, seq_len|\n",
    "        output = F.max_pool1d(output, output.size(2)).squeeze(2)\n",
    "        # output = |bs, hidden_size_linear|\n",
    "        output = self.fc(output)\n",
    "        # output = |bs, class_num|\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RCNN(vocab_size=50265, embedding_dim=768, hidden_size=512, hidden_size_linear=512, class_num = 2, dropout = 0.1).to(device)\n",
    "\n",
    "batch = next(iter(train_iterator))\n",
    "        \n",
    "ids = batch['ids'].to(DEVICE, dtype = torch.long)\n",
    "mask = batch['mask'].to(DEVICE, dtype = torch.long)\n",
    "token_type_ids = batch['token_type_ids'].to(DEVICE, dtype = torch.long)\n",
    "targets = batch['targets'].to(DEVICE, dtype = torch.long)\n",
    "\n",
    "# print(ids)\n",
    "# print(mask)\n",
    "# print(token_type_ids)\n",
    "print(targets)\n",
    "print(ids)\n",
    "# outputs = model(ids)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 97\n",
    "sarc_path = '/home/tegzes/Desktop/FL-Detection-Experiments/datamodule/isarcasm2022.csv'\n",
    "BATCH_SIZE_TRAIN = 8\n",
    "BATCH_SIZE_TEST = 4\n",
    "MAX_LEN = 256\n",
    "HIDDEN_DIM = 64\n",
    "OUTPUT_DIM = 1\n",
    "EMBEDDING_LENGTH = 300\n",
    "N_LAYERS = 2\n",
    "LEARNING_RATE = 1e-5\n",
    "BIDIRECTIONAL = False\n",
    "DROPOUT = 0.25\n",
    "N_EPOCHS = 3\n",
    "utils.seed_everything(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = data.roberta_data_loader(sarc_path, BATCH_SIZE_TRAIN, BATCH_SIZE_TEST, True, 0, MAX_LEN, bert_tokenizer, SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iterator))\n",
    "\n",
    "print(batch)    \n",
    "ids = batch['ids'].to(DEVICE, dtype = torch.long)\n",
    "mask = batch['mask'].to(DEVICE, dtype = torch.long)\n",
    "token_type_ids = batch['token_type_ids'].to(DEVICE, dtype = torch.long)\n",
    "targets = batch['targets'].to(DEVICE, dtype = torch.long)\n",
    "\n",
    "# print(ids)\n",
    "# print(mask)\n",
    "# print(token_type_ids)\n",
    "print(targets)\n",
    "outputs = model(ids, mask, token_type_ids)\n",
    "print(outputs)\n",
    "# _, predictions = torch.max(outputs.data, dim = 1)\n",
    "# print(predictions)\n",
    "# loss = cross_entropy_loss(outputs, targets)\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "bce_loss = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "def calcuate_accuracy(preds, targets):\n",
    "    n_correct = (preds==targets).sum().item()\n",
    "    return n_correct\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task = Task.init(project_name=\"FL Detection\", task_name=\"Training\")\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    no_of_iterations = 0\n",
    "    no_of_examples = 0\n",
    "    \n",
    "    # using torch metrics to calculate the metrics\n",
    "    metric_acc = torchmetrics.Accuracy().to(torch.device(\"cuda\", 0))\n",
    "    metric_f1 = torchmetrics.F1(num_classes = 2, average=\"none\").to(torch.device(\"cuda\", 0))\n",
    "    metric_f1_micro = torchmetrics.F1(num_classes = 2).to(torch.device(\"cuda\", 0))\n",
    "    metric_f1_macro = torchmetrics.F1(num_classes = 2, average='macro').to(torch.device(\"cuda\", 0))\n",
    "    metric_precision = torchmetrics.Precision(num_classes = 2, average=\"none\").to(torch.device(\"cuda\", 0))\n",
    "    metric_recall = torchmetrics.Recall(num_classes = 2, average=\"none\").to(torch.device(\"cuda\", 0))\n",
    "  \n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, batch in tqdm(enumerate(iterator, 0)):\n",
    "        \n",
    "        ids = batch['ids'].to(DEVICE, dtype = torch.long)\n",
    "        mask = batch['mask'].to(DEVICE, dtype = torch.long)\n",
    "        token_type_ids = batch['token_type_ids'].to(DEVICE, dtype = torch.long)\n",
    "        targets = batch['targets'].to(DEVICE, dtype = torch.long)\n",
    "        # tweet_lens = batch['tweet_len']\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)#, tweet_lens)#.to('cpu'))\n",
    "        # outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        # targets = targets.unsqueeze(1) # for BCEWithLogitsLoss criterion\n",
    "        loss = criterion(outputs, targets)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        _, predictions = torch.max(outputs.data, dim = 1)\n",
    "        acc = calcuate_accuracy(predictions, targets)\n",
    "\n",
    "        # loss = criterion(predictions, targets)\n",
    "        # epoch_loss += loss.item()\n",
    "\n",
    "        no_of_iterations += 1\n",
    "        no_of_examples += targets.size(0)\n",
    "                \n",
    "        metric_acc.update(predictions, targets)\n",
    "        metric_f1.update(outputs, targets)\n",
    "        metric_f1_micro.update(outputs, targets)\n",
    "        metric_f1_macro.update(outputs, targets)\n",
    "        metric_precision.update(predictions, targets)\n",
    "        metric_recall.update(predictions, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # for GPU\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logger.current_logger().report_scalar(\n",
    "        #     \"train\", \"loss\", iteration = (epoch * len(iterator) + batch_idx), value = loss.item())\n",
    "#----\n",
    "        # print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        #         epoch, batch_idx * len(batch['ids']), len(iterator),\n",
    "        #         100. * batch_idx / len(iterator), loss.item()))\n",
    "        \n",
    "\n",
    "    epoch_loss = epoch_loss/no_of_iterations\n",
    "    epoch_acc = (acc*100)/no_of_examples\n",
    "        \n",
    "    acc_torch = metric_acc.compute()\n",
    "    print(f\"Training Accuracy: {acc_torch}\")\n",
    "    \n",
    "    f1 = metric_f1.compute()\n",
    "    print(f\"Training F1: {f1}\")\n",
    "    \n",
    "    f1_micro = metric_f1_micro.compute()\n",
    "    print(f\"Training F1 Micro: {f1_micro}\")\n",
    "\n",
    "    f1_macro = metric_f1_macro.compute()\n",
    "    print(f\"Training F1 Macro: {f1_macro}\")\n",
    " \n",
    "    precision = metric_precision.compute()\n",
    "    print(f\"Training Precision: {precision}\")\n",
    "\n",
    "    recall = metric_recall.compute()\n",
    "    print(f\"Training Recall: {recall}\")\n",
    "    \n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "  \n",
    "    # Logger.current_logger().report_scalar(\n",
    "    #     \"train\", \"accuracy\", iteration=epoch, value=acc_torch)\n",
    "\n",
    "    \n",
    "    metric_acc.reset()\n",
    "    metric_f1.reset()\n",
    "    metric_f1_micro.reset()\n",
    "    metric_f1_macro.reset()\n",
    "    metric_precision.reset()\n",
    "    metric_recall.reset()\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# evaluation routine\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    no_of_iterations = 0\n",
    "    no_of_examples = 0    \n",
    "    \n",
    "   # torch metrics\n",
    "    metric_acc = torchmetrics.Accuracy().to(torch.device(\"cuda\", 0))\n",
    "    metric_f1 = torchmetrics.F1(num_classes = 2, average=\"none\").to(torch.device(\"cuda\", 0))\n",
    "    metric_f1_micro = torchmetrics.F1(num_classes = 2).to(torch.device(\"cuda\", 0))\n",
    "    metric_f1_macro = torchmetrics.F1(num_classes = 2, average='macro').to(torch.device(\"cuda\", 0))\n",
    "    metric_precision = torchmetrics.Precision(num_classes = 2, average=\"none\").to(torch.device(\"cuda\", 0))\n",
    "    metric_recall = torchmetrics.Recall(num_classes = 2, average=\"none\").to(torch.device(\"cuda\", 0))\n",
    "  \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for _, batch in tqdm(enumerate(iterator, 0)):\n",
    "\n",
    "            ids = batch['ids'].to(DEVICE, dtype = torch.long)\n",
    "            mask = batch['mask'].to(DEVICE, dtype = torch.long)\n",
    "            token_type_ids = batch['token_type_ids'].to(DEVICE, dtype = torch.long)\n",
    "            targets = batch['targets'].to(DEVICE, dtype = torch.long)\n",
    "\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "        \n",
    "            _, predictions = torch.max(outputs.data, dim = 1)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            acc = calcuate_accuracy(predictions, targets)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "            metric_acc.update(predictions, targets)\n",
    "            metric_f1.update(outputs, targets)\n",
    "            metric_f1_micro.update(outputs, targets)\n",
    "            metric_f1_macro.update(outputs, targets)\n",
    "            metric_precision.update(predictions, targets)\n",
    "            metric_recall.update(predictions, targets)\n",
    "\n",
    "            no_of_iterations += 1\n",
    "            no_of_examples += targets.size(0)\n",
    "    \n",
    "    epoch_loss = epoch_loss/no_of_iterations\n",
    "    epoch_acc = (acc*100)/no_of_iterations #no_of_examples\n",
    "\n",
    "    # print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    #     epoch_loss, acc, len(iterator),\n",
    "    #     100. * acc / len(iterator)))\n",
    "\n",
    "    acc_torch = metric_acc.compute()\n",
    "    print(f\"Validation Accuracy: {acc_torch}\")\n",
    "    \n",
    "    f1 = metric_f1.compute()\n",
    "    print(f\"Validation F1 Validation: {f1}\")\n",
    "    \n",
    "    f1_micro = metric_f1_micro.compute()\n",
    "    print(f\"Validation F1 Micro: {f1_micro}\")\n",
    "\n",
    "    f1_macro = metric_f1_macro.compute()\n",
    "    print(f\"Validation F1 Macro: {f1_macro}\")\n",
    " \n",
    "    precision = metric_precision.compute()\n",
    "    print(f\"Validation Precision: {precision}\")\n",
    "\n",
    "    recall = metric_recall.compute()\n",
    "    print(f\"Validation Recall: {recall}\")\n",
    "    \n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "\n",
    "    # clear ml\n",
    "    # Logger.current_logger().report_scalar(\n",
    "    #     \"test\", \"loss\", iteration=epoch, value=epoch_loss)\n",
    "    # Logger.current_logger().report_scalar(\n",
    "    #     \"test\", \"accuracy\", iteration=epoch, value=acc_torch)\n",
    "\n",
    "    metric_acc.reset()\n",
    "    metric_f1.reset()\n",
    "    metric_f1_micro.reset()\n",
    "    metric_f1_macro.reset()\n",
    "    metric_precision.reset()\n",
    "    metric_recall.reset()\n",
    "             \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "# experiment loop\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, cross_entropy_loss)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, cross_entropy_loss)\n",
    "        \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, cross_entropy_loss)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "\n",
    "# task.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(bert(**bert_tokens)[0].shape)\n",
    "# pooler = hidden_state[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BertClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertClass, self).__init__()\n",
    "        self.l1 = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.25)\n",
    "        self.classifier = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[1]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output\n",
    "\n",
    "\n",
    "bert_model = BertClass()\n",
    "bert_model.to(DEVICE)\n",
    "\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "bce_loss = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(bert_model.parameters(), lr = LEARNING_RATE)\n",
    "def calcuate_accuracy(preds, targets):\n",
    "    n_correct = (preds==targets).sum().item()\n",
    "    return n_correct\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    no_of_iterations = 0\n",
    "    no_of_examples = 0\n",
    "    \n",
    "    # using torch metrics to calculate the metrics\n",
    "    metric_acc = torchmetrics.Accuracy().to(torch.device(\"cuda\", 0))\n",
    "    metric_f1 = torchmetrics.F1(num_classes = 2, average=\"none\").to(torch.device(\"cuda\", 0))\n",
    "    metric_f1_micro = torchmetrics.F1(num_classes = 2).to(torch.device(\"cuda\", 0))\n",
    "    metric_f1_macro = torchmetrics.F1(num_classes = 2, average='macro').to(torch.device(\"cuda\", 0))\n",
    "    metric_precision = torchmetrics.Precision(num_classes = 2, average=\"none\").to(torch.device(\"cuda\", 0))\n",
    "    metric_recall = torchmetrics.Recall(num_classes = 2, average=\"none\").to(torch.device(\"cuda\", 0))\n",
    "  \n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, batch in tqdm(enumerate(iterator, 0)):\n",
    "        \n",
    "        ids = batch['ids'].to(DEVICE, dtype = torch.long)\n",
    "        mask = batch['mask'].to(DEVICE, dtype = torch.long)\n",
    "        token_type_ids = batch['token_type_ids'].to(DEVICE, dtype = torch.long)\n",
    "        targets = batch['targets'].to(DEVICE, dtype = torch.long)\n",
    "        # tweet_lens = batch['tweet_len']\n",
    "\n",
    "#         outputs = model(ids)#, tweet_lens)#.to('cpu'))\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        # targets = targets.unsqueeze(1) # for BCEWithLogitsLoss criterion\n",
    "        loss = criterion(outputs, targets)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        _, predictions = torch.max(outputs.data, dim = 1)\n",
    "        acc = calcuate_accuracy(predictions, targets)\n",
    "\n",
    "        # loss = criterion(predictions, targets)\n",
    "        # epoch_loss += loss.item()\n",
    "\n",
    "        no_of_iterations += 1\n",
    "        no_of_examples += targets.size(0)\n",
    "                \n",
    "        metric_acc.update(predictions, targets)\n",
    "        metric_f1.update(outputs, targets)\n",
    "        metric_f1_micro.update(outputs, targets)\n",
    "        metric_f1_macro.update(outputs, targets)\n",
    "        metric_precision.update(predictions, targets)\n",
    "        metric_recall.update(predictions, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # for GPU\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logger.current_logger().report_scalar(\n",
    "        #     \"train\", \"loss\", iteration = (epoch * len(iterator) + batch_idx), value = loss.item())\n",
    "#----\n",
    "        # print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        #         epoch, batch_idx * len(batch['ids']), len(iterator),\n",
    "        #         100. * batch_idx / len(iterator), loss.item()))\n",
    "        \n",
    "\n",
    "    epoch_loss = epoch_loss/no_of_iterations\n",
    "    epoch_acc = (acc*100)/no_of_examples\n",
    "        \n",
    "    acc_torch = metric_acc.compute()\n",
    "    print(f\"Training Accuracy: {acc_torch}\")\n",
    "    \n",
    "    f1 = metric_f1.compute()\n",
    "    print(f\"Training F1: {f1}\")\n",
    "    \n",
    "    f1_micro = metric_f1_micro.compute()\n",
    "    print(f\"Training F1 Micro: {f1_micro}\")\n",
    "\n",
    "    f1_macro = metric_f1_macro.compute()\n",
    "    print(f\"Training F1 Macro: {f1_macro}\")\n",
    " \n",
    "    precision = metric_precision.compute()\n",
    "    print(f\"Training Precision: {precision}\")\n",
    "\n",
    "    recall = metric_recall.compute()\n",
    "    print(f\"Training Recall: {recall}\")\n",
    "    \n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "  \n",
    "    # Logger.current_logger().report_scalar(\n",
    "    #     \"train\", \"accuracy\", iteration=epoch, value=acc_torch)\n",
    "\n",
    "    \n",
    "    metric_acc.reset()\n",
    "    metric_f1.reset()\n",
    "    metric_f1_micro.reset()\n",
    "    metric_f1_macro.reset()\n",
    "    metric_precision.reset()\n",
    "    metric_recall.reset()\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# evaluation routine\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    no_of_iterations = 0\n",
    "    no_of_examples = 0    \n",
    "    \n",
    "   # torch metrics\n",
    "    metric_acc = torchmetrics.Accuracy().to(torch.device(\"cuda\", 0))\n",
    "    metric_f1 = torchmetrics.F1(num_classes = 2, average=\"none\").to(torch.device(\"cuda\", 0))\n",
    "    metric_f1_micro = torchmetrics.F1(num_classes = 2).to(torch.device(\"cuda\", 0))\n",
    "    metric_f1_macro = torchmetrics.F1(num_classes = 2, average='macro').to(torch.device(\"cuda\", 0))\n",
    "    metric_precision = torchmetrics.Precision(num_classes = 2, average=\"none\").to(torch.device(\"cuda\", 0))\n",
    "    metric_recall = torchmetrics.Recall(num_classes = 2, average=\"none\").to(torch.device(\"cuda\", 0))\n",
    "  \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for _, batch in tqdm(enumerate(iterator, 0)):\n",
    "\n",
    "            ids = batch['ids'].to(DEVICE, dtype = torch.long)\n",
    "            mask = batch['mask'].to(DEVICE, dtype = torch.long)\n",
    "            token_type_ids = batch['token_type_ids'].to(DEVICE, dtype = torch.long)\n",
    "            targets = batch['targets'].to(DEVICE, dtype = torch.long)\n",
    "\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "        \n",
    "            _, predictions = torch.max(outputs.data, dim = 1)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            acc = calcuate_accuracy(predictions, targets)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "            metric_acc.update(predictions, targets)\n",
    "            metric_f1.update(outputs, targets)\n",
    "            metric_f1_micro.update(outputs, targets)\n",
    "            metric_f1_macro.update(outputs, targets)\n",
    "            metric_precision.update(predictions, targets)\n",
    "            metric_recall.update(predictions, targets)\n",
    "\n",
    "            no_of_iterations += 1\n",
    "            no_of_examples += targets.size(0)\n",
    "    \n",
    "    epoch_loss = epoch_loss/no_of_iterations\n",
    "    epoch_acc = (acc*100)/no_of_iterations #no_of_examples\n",
    "\n",
    "    # print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    #     epoch_loss, acc, len(iterator),\n",
    "    #     100. * acc / len(iterator)))\n",
    "\n",
    "    acc_torch = metric_acc.compute()\n",
    "    print(f\"Validation Accuracy: {acc_torch}\")\n",
    "    \n",
    "    f1 = metric_f1.compute()\n",
    "    print(f\"Validation F1 Validation: {f1}\")\n",
    "    \n",
    "    f1_micro = metric_f1_micro.compute()\n",
    "    print(f\"Validation F1 Micro: {f1_micro}\")\n",
    "\n",
    "    f1_macro = metric_f1_macro.compute()\n",
    "    print(f\"Validation F1 Macro: {f1_macro}\")\n",
    " \n",
    "    precision = metric_precision.compute()\n",
    "    print(f\"Validation Precision: {precision}\")\n",
    "\n",
    "    recall = metric_recall.compute()\n",
    "    print(f\"Validation Recall: {recall}\")\n",
    "    \n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "\n",
    "    # clear ml\n",
    "    # Logger.current_logger().report_scalar(\n",
    "    #     \"test\", \"loss\", iteration=epoch, value=epoch_loss)\n",
    "    # Logger.current_logger().report_scalar(\n",
    "    #     \"test\", \"accuracy\", iteration=epoch, value=acc_torch)\n",
    "\n",
    "    metric_acc.reset()\n",
    "    metric_f1.reset()\n",
    "    metric_f1_micro.reset()\n",
    "    metric_f1_macro.reset()\n",
    "    metric_precision.reset()\n",
    "    metric_recall.reset()\n",
    "             \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "for epoch in range(3):\n",
    "\n",
    "    train_loss, _ = train(bert_model, train_iterator, optimizer, cross_entropy_loss)\n",
    "    valid_loss, _ = evaluate(bert_model, valid_iterator, cross_entropy_loss)\n",
    "        \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} ')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} ')\n",
    "\n",
    "\n",
    "test_loss, _ = evaluate(bert_model, test_iterator, cross_entropy_loss)\n",
    "print(f'Test Loss: {test_loss:.3f} ')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "353d8282a16112a98a41145f3f9c1bed3cd5174dd47df6cfdaac153128affcbe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
