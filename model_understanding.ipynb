{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "\n",
    "from tqdm import tqdm \n",
    "import hydra\n",
    "# from omegaconf import DictConfig\n",
    "\n",
    "from clearml import Task, Logger\n",
    "\n",
    "from datamodule import data, utils\n",
    "from models import LSTM, CNN, Bertweet, Roberta\n",
    "\n",
    "from transformers import BertModel\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1999, 7143, 2342, 1997, 1998, 1045, 2064, 2025, 6911, 2023,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(\"in desperate need of and i can not stress this enough spring break\", padding='max_length', max_length=12, truncation=True, return_tensors='pt')  \n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BERTSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 output_dim):\n",
    "        super(BERTSentiment, self).__init__()\n",
    "\n",
    "        self.bert = bert\n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        self.LSTM = torch.nn.LSTM(768, 384, batch_first=True, bidirectional=True)\n",
    "        self.out = nn.Linear(768, output_dim)\n",
    "\n",
    "    def forward(self, text, mask):\n",
    "        #text = [batch size, sent len]\n",
    "        # seq_layers, pooled_output = self.bert(text)\n",
    "        embedded = self.bert(text, mask)[0]\n",
    "\n",
    "        # embedded = embedded[1]\n",
    "        lstm_output, (last_hidden, _) = self.LSTM(embedded)\n",
    "        #embedded = [batch size, emb dim]\n",
    "\n",
    "        output_hidden = torch.cat((last_hidden[0], last_hidden[1]), dim=-1)\n",
    "        output_hidden = F.dropout(output_hidden,0.2)\n",
    "        \n",
    "        # output_hidden = F.dropout(embedded, 0.2)\n",
    "        output = self.out(output_hidden)\n",
    "\n",
    "        #output = [batch size, out dim]\n",
    "        return output\n",
    "\n",
    "        \n",
    "model = BERTSentiment(bert, 2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=aff2ccdbba2e4a31ad9235bcdfc43e1d\n",
      "ClearML new version available: upgrade to v1.3.2 is recommended!\n",
      "ClearML results page: https://app.clear.ml/projects/dbb746eee2fa4a9485a64c341e357772/experiments/aff2ccdbba2e4a31ad9235bcdfc43e1d/output/log\n",
      "tensor([[ 0.9134, -0.9969]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "ClearML Monitor: Could not detect iteration reporting, falling back to iterations as seconds-from-start\n"
     ]
    }
   ],
   "source": [
    "# print(model(tokens.input_ids, tokens.attention_mask))\n",
    "input_ids = tokens.input_ids.to(device)\n",
    "mask = tokens.attention_mask.to(device)\n",
    "print(model(input_ids, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 97\n",
    "sarc_path = '/home/tegzes/Desktop/FL-Detection-Experiments/datamodule/isarcasm2022.csv'\n",
    "BATCH_SIZE_TRAIN = 1\n",
    "BATCH_SIZE_TEST = 1\n",
    "MAX_LEN = 256\n",
    "HIDDEN_DIM = 64\n",
    "OUTPUT_DIM = 1\n",
    "EMBEDDING_LENGTH = 300\n",
    "N_LAYERS = 2\n",
    "LEARNING_RATE = 1e-5\n",
    "BIDIRECTIONAL = False\n",
    "DROPOUT = 0.25\n",
    "N_EPOCHS = 3\n",
    "utils.seed_everything(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = data.roberta_data_loader(sarc_path, BATCH_SIZE_TRAIN, BATCH_SIZE_TEST, True, 0, MAX_LEN, tokenizer, SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "bce_loss = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "def calcuate_accuracy(preds, targets):\n",
    "    n_correct = (preds==targets).sum().item()\n",
    "    return n_correct\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/tegzes/anaconda3/envs/ml/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "2219it [10:43,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7458314299583435\n",
      "Training F1: tensor([0.8537, 0.0309], device='cuda:0')\n",
      "Training F1 Micro: 0.7458314299583435\n",
      "Training F1 Macro: 0.4423311650753021\n",
      "Training Precision: tensor([0.7478, 0.5000], device='cuda:0')\n",
      "Training Recall: tensor([0.9946, 0.0160], device='cuda:0')\n",
      "Training Loss Epoch: 0.5568178544503416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "555it [00:31, 17.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.745945930480957\n",
      "Validation F1 Validation: tensor([0.8505, 0.1557], device='cuda:0')\n",
      "Validation F1 Micro: 0.745945930480957\n",
      "Validation F1 Macro: 0.5030829310417175\n",
      "Validation Precision: tensor([0.7624, 0.4483], device='cuda:0')\n",
      "Validation Recall: tensor([0.9616, 0.0942], device='cuda:0')\n",
      "Validation Loss Epoch: 0.5333852529257267\n",
      "Epoch: 01\n",
      "\tTrain Loss: 0.557 | Train Acc: 0.00%\n",
      "\t Val. Loss: 0.533 |  Val. Acc: 18.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2219it [10:37,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8179360032081604\n",
      "Training F1: tensor([0.8840, 0.5765], device='cuda:0')\n",
      "Training F1 Micro: 0.8179360032081604\n",
      "Training F1 Macro: 0.7302806377410889\n",
      "Training Precision: tensor([0.8420, 0.7051], device='cuda:0')\n",
      "Training Recall: tensor([0.9305, 0.4876], device='cuda:0')\n",
      "Training Loss Epoch: 0.4001772655736539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "555it [00:30, 18.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7387387156486511\n",
      "Validation F1 Validation: tensor([0.8387, 0.3128], device='cuda:0')\n",
      "Validation F1 Micro: 0.7387387156486511\n",
      "Validation F1 Macro: 0.5757529139518738\n",
      "Validation Precision: tensor([0.7822, 0.4521], device='cuda:0')\n",
      "Validation Recall: tensor([0.9041, 0.2391], device='cuda:0')\n",
      "Validation Loss Epoch: 0.6120118013104877\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.400 | Train Acc: 4.51%\n",
      "\t Val. Loss: 0.612 |  Val. Acc: 18.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2219it [10:34,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9504281282424927\n",
      "Training F1: tensor([0.9668, 0.9018], device='cuda:0')\n",
      "Training F1 Micro: 0.9504281282424927\n",
      "Training F1 Macro: 0.9343165755271912\n",
      "Training Precision: tensor([0.9645, 0.9083], device='cuda:0')\n",
      "Training Recall: tensor([0.9692, 0.8954], device='cuda:0')\n",
      "Training Loss Epoch: 0.14204120561662045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "555it [00:30, 17.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.73153156042099\n",
      "Validation F1 Validation: tensor([0.8285, 0.3817], device='cuda:0')\n",
      "Validation F1 Micro: 0.73153156042099\n",
      "Validation F1 Macro: 0.6051406264305115\n",
      "Validation Precision: tensor([0.7965, 0.4466], device='cuda:0')\n",
      "Validation Recall: tensor([0.8633, 0.3333], device='cuda:0')\n",
      "Validation Loss Epoch: 0.8714719046582677\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.142 | Train Acc: 4.51%\n",
      "\t Val. Loss: 0.871 |  Val. Acc: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "694it [00:38, 18.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7420749068260193\n",
      "Validation F1 Validation: tensor([0.8397, 0.3395], device='cuda:0')\n",
      "Validation F1 Micro: 0.7420749068260193\n",
      "Validation F1 Macro: 0.5896163582801819\n",
      "Validation Precision: tensor([0.7976, 0.4340], device='cuda:0')\n",
      "Validation Recall: tensor([0.8866, 0.2788], device='cuda:0')\n",
      "Validation Loss Epoch: 0.8915217733830925\n",
      "Test Loss: 0.892 | Test Acc: 14.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "task = Task.init(project_name=\"FL Detection\", task_name=\"Training\")\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    no_of_iterations = 0\n",
    "    no_of_examples = 0\n",
    "    \n",
    "    # using torch metrics to calculate the metrics\n",
    "    metric_acc = torchmetrics.Accuracy().to(torch.device(\"cuda\", 0))\n",
    "    metric_f1 = torchmetrics.F1(num_classes = 2, average=\"none\").to(torch.device(\"cuda\", 0))\n",
    "    metric_f1_micro = torchmetrics.F1(num_classes = 2).to(torch.device(\"cuda\", 0))\n",
    "    metric_f1_macro = torchmetrics.F1(num_classes = 2, average='macro').to(torch.device(\"cuda\", 0))\n",
    "    metric_precision = torchmetrics.Precision(num_classes = 2, average=\"none\").to(torch.device(\"cuda\", 0))\n",
    "    metric_recall = torchmetrics.Recall(num_classes = 2, average=\"none\").to(torch.device(\"cuda\", 0))\n",
    "  \n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, batch in tqdm(enumerate(iterator, 0)):\n",
    "        \n",
    "        ids = batch['ids'].to(DEVICE, dtype = torch.long)\n",
    "        mask = batch['mask'].to(DEVICE, dtype = torch.long)\n",
    "        token_type_ids = batch['token_type_ids'].to(DEVICE, dtype = torch.long)\n",
    "        targets = batch['targets'].to(DEVICE, dtype = torch.long)\n",
    "        # tweet_lens = batch['tweet_len']\n",
    "\n",
    "        outputs = model(ids, mask)#, tweet_lens)#.to('cpu'))\n",
    "        # outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        # targets = targets.unsqueeze(1) # for BCEWithLogitsLoss criterion\n",
    "        loss = criterion(outputs, targets)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        _, predictions = torch.max(outputs.data, dim = 1)\n",
    "        acc = calcuate_accuracy(predictions, targets)\n",
    "\n",
    "        # loss = criterion(predictions, targets)\n",
    "        # epoch_loss += loss.item()\n",
    "\n",
    "        no_of_iterations += 1\n",
    "        no_of_examples += targets.size(0)\n",
    "                \n",
    "        metric_acc.update(predictions, targets)\n",
    "        metric_f1.update(outputs, targets)\n",
    "        metric_f1_micro.update(outputs, targets)\n",
    "        metric_f1_macro.update(outputs, targets)\n",
    "        metric_precision.update(predictions, targets)\n",
    "        metric_recall.update(predictions, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # for GPU\n",
    "        optimizer.step()\n",
    "\n",
    "        Logger.current_logger().report_scalar(\n",
    "            \"train\", \"loss\", iteration = (epoch * len(iterator) + batch_idx), value = loss.item())\n",
    "#----\n",
    "        # print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        #         epoch, batch_idx * len(batch['ids']), len(iterator),\n",
    "        #         100. * batch_idx / len(iterator), loss.item()))\n",
    "        \n",
    "\n",
    "    epoch_loss = epoch_loss/no_of_iterations\n",
    "    epoch_acc = (acc*100)/no_of_examples\n",
    "        \n",
    "    acc_torch = metric_acc.compute()\n",
    "    print(f\"Training Accuracy: {acc_torch}\")\n",
    "    \n",
    "    f1 = metric_f1.compute()\n",
    "    print(f\"Training F1: {f1}\")\n",
    "    \n",
    "    f1_micro = metric_f1_micro.compute()\n",
    "    print(f\"Training F1 Micro: {f1_micro}\")\n",
    "\n",
    "    f1_macro = metric_f1_macro.compute()\n",
    "    print(f\"Training F1 Macro: {f1_macro}\")\n",
    " \n",
    "    precision = metric_precision.compute()\n",
    "    print(f\"Training Precision: {precision}\")\n",
    "\n",
    "    recall = metric_recall.compute()\n",
    "    print(f\"Training Recall: {recall}\")\n",
    "    \n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "  \n",
    "    Logger.current_logger().report_scalar(\n",
    "        \"train\", \"accuracy\", iteration=epoch, value=acc_torch)\n",
    "\n",
    "    \n",
    "    metric_acc.reset()\n",
    "    metric_f1.reset()\n",
    "    metric_f1_micro.reset()\n",
    "    metric_f1_macro.reset()\n",
    "    metric_precision.reset()\n",
    "    metric_recall.reset()\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# evaluation routine\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    no_of_iterations = 0\n",
    "    no_of_examples = 0    \n",
    "    \n",
    "   # torch metrics\n",
    "    metric_acc = torchmetrics.Accuracy().to(torch.device(\"cuda\", 0))\n",
    "    metric_f1 = torchmetrics.F1(num_classes = 2, average=\"none\").to(torch.device(\"cuda\", 0))\n",
    "    metric_f1_micro = torchmetrics.F1(num_classes = 2).to(torch.device(\"cuda\", 0))\n",
    "    metric_f1_macro = torchmetrics.F1(num_classes = 2, average='macro').to(torch.device(\"cuda\", 0))\n",
    "    metric_precision = torchmetrics.Precision(num_classes = 2, average=\"none\").to(torch.device(\"cuda\", 0))\n",
    "    metric_recall = torchmetrics.Recall(num_classes = 2, average=\"none\").to(torch.device(\"cuda\", 0))\n",
    "  \n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for _, batch in tqdm(enumerate(iterator, 0)):\n",
    "\n",
    "            ids = batch['ids'].to(DEVICE, dtype = torch.long)\n",
    "            mask = batch['mask'].to(DEVICE, dtype = torch.long)\n",
    "            token_type_ids = batch['token_type_ids'].to(DEVICE, dtype = torch.long)\n",
    "            targets = batch['targets'].to(DEVICE, dtype = torch.long)\n",
    "\n",
    "            outputs = model(ids, mask)\n",
    "        \n",
    "            _, predictions = torch.max(outputs.data, dim = 1)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            acc = calcuate_accuracy(predictions, targets)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "            metric_acc.update(predictions, targets)\n",
    "            metric_f1.update(outputs, targets)\n",
    "            metric_f1_micro.update(outputs, targets)\n",
    "            metric_f1_macro.update(outputs, targets)\n",
    "            metric_precision.update(predictions, targets)\n",
    "            metric_recall.update(predictions, targets)\n",
    "\n",
    "            no_of_iterations += 1\n",
    "            no_of_examples += targets.size(0)\n",
    "    \n",
    "    epoch_loss = epoch_loss/no_of_iterations\n",
    "    epoch_acc = (acc*100)/no_of_iterations #no_of_examples\n",
    "\n",
    "    # print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    #     epoch_loss, acc, len(iterator),\n",
    "    #     100. * acc / len(iterator)))\n",
    "\n",
    "    acc_torch = metric_acc.compute()\n",
    "    print(f\"Validation Accuracy: {acc_torch}\")\n",
    "    \n",
    "    f1 = metric_f1.compute()\n",
    "    print(f\"Validation F1 Validation: {f1}\")\n",
    "    \n",
    "    f1_micro = metric_f1_micro.compute()\n",
    "    print(f\"Validation F1 Micro: {f1_micro}\")\n",
    "\n",
    "    f1_macro = metric_f1_macro.compute()\n",
    "    print(f\"Validation F1 Macro: {f1_macro}\")\n",
    " \n",
    "    precision = metric_precision.compute()\n",
    "    print(f\"Validation Precision: {precision}\")\n",
    "\n",
    "    recall = metric_recall.compute()\n",
    "    print(f\"Validation Recall: {recall}\")\n",
    "    \n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "\n",
    "    # clear ml\n",
    "    Logger.current_logger().report_scalar(\n",
    "        \"test\", \"loss\", iteration=epoch, value=epoch_loss)\n",
    "    Logger.current_logger().report_scalar(\n",
    "        \"test\", \"accuracy\", iteration=epoch, value=acc_torch)\n",
    "\n",
    "    metric_acc.reset()\n",
    "    metric_f1.reset()\n",
    "    metric_f1_micro.reset()\n",
    "    metric_f1_macro.reset()\n",
    "    metric_precision.reset()\n",
    "    metric_recall.reset()\n",
    "             \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "# experiment loop\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, cross_entropy_loss)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, cross_entropy_loss)\n",
    "        \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, cross_entropy_loss)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "353d8282a16112a98a41145f3f9c1bed3cd5174dd47df6cfdaac153128affcbe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
